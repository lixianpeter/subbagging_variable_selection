{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06dc05d4-2cf9-4a72-a1fd-5b92ff400edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#rep_ind_current = 1\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# The following are the commonly used packages\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "import random \n",
    "import pandas as pd\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f27a4a1-344a-4542-bf70-13051d4ccebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some loss functions\n",
    "\n",
    "def adaptive(beta,f,y,x,beta_0,lamda):\n",
    "    return f(beta,y,x)+lamda*sum(abs(beta)/abs(beta_0))\n",
    "\n",
    "# Numerical derivatives for testing\n",
    "def first_derivative(f,beta,*args):\n",
    "    grad=np.zeros(len(beta))\n",
    "    i=0\n",
    "    for coef in beta:\n",
    "        h=np.zeros(len(beta))\n",
    "        h[i]=1e-4\n",
    "        grad[i]=(f(beta+h,*args)-f(beta,*args))/1e-4\n",
    "        i+=1\n",
    "    return grad\n",
    "    \n",
    "def second_derivative(f,beta,*args):\n",
    "    hess=[]\n",
    "    for i in range(0,len(beta)):\n",
    "        h=np.zeros(len(beta))\n",
    "        h[i]=1e-4\n",
    "        hess+=[((first_derivative(f,beta+h,*args)-first_derivative(f,beta,*args))/1e-4)]\n",
    "    return np.stack(hess)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e0f1c2-cd0d-41ae-b385-2afcf1841ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For linear regression\n",
    "def mse(beta,y,x):\n",
    "    return sum((y-x@beta)**2)/len(y)\n",
    "    \n",
    "def mse_first_derivative(beta,y,x):\n",
    "    return -2*(x.T@(y-x@beta))/len(y)\n",
    "\n",
    "def mse_first_derivative_per_obs(beta, y, x):\n",
    "    r = y - x @ beta                    \n",
    "    return -2 * x * r[:, None]         \n",
    "    \n",
    "def mse_second_derivative(beta,y,x):\n",
    "    return 2*x.T@x/len(y)\n",
    "\n",
    "# For logistic regression\n",
    "# def logistic(z):\n",
    "#     return np.exp(z) / (1 + np.exp(z))\n",
    "    \n",
    "# def logistic_likelihood(beta,y,x):\n",
    "#     X=x\n",
    "#     Y=y\n",
    "#     p=logistic(X@beta)\n",
    "#     return -sum((Y*np.log(p)+(1-Y)*np.log(1-p)))/len(y)\n",
    "\n",
    "# def logistic_first_derivative(beta,y,x):\n",
    "#     Y=y\n",
    "#     X=x\n",
    "#     p=logistic(X@beta)\n",
    "#     return -(Y@X-p@X)/len(y)\n",
    "\n",
    "# def logistic_first_derivative_per_obs(beta, y, x):\n",
    "#     X = x\n",
    "#     Y = y\n",
    "#     p = logistic(X @ beta)          \n",
    "#     return X * (p - Y)[:, None]    \n",
    "    \n",
    "# def logistic_second_derivative(beta,y,x):\n",
    "#     Y=y\n",
    "#     X=x\n",
    "#     p=logistic(X@beta)\n",
    "#     return X.transpose()*(p*(1-p))@X/len(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b2adb6a-3e71-420d-bda8-9461db4b41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is for a subsample's estimation\n",
    "\n",
    "def subsample_estimate(subsample, f, beta_true = None): #  f is the loss; p is the dimension\n",
    "\n",
    "    # extract one subsample\n",
    "    #simu_data = pd.read_csv(file_name, header = 0).to_numpy()\n",
    "    y_subsample = subsample[:,0]\n",
    "    x_subsample = subsample[:,1:]\n",
    "    p = x_subsample.shape[1]\n",
    "    if (beta_true.all() == None):\n",
    "        beta_true = np.zeros(p)\n",
    "    k_N = len(y_subsample)\n",
    "\n",
    "    \n",
    "    # Obtain subsample estimates\n",
    "    beta_subsample = np.linalg.inv(x_subsample.T@x_subsample)@x_subsample.T@y_subsample\n",
    "    #minimize(f, beta_true, method = 'BFGS',    \n",
    "                            #args = (y_subsample,x_subsample)).x\n",
    "        # (x.tx)^(-1)xy\n",
    "    # Obtain capital Sigma for linear regression\n",
    "    second_derivative_subsample = mse_second_derivative(beta_subsample, y_subsample, x_subsample)\n",
    "\n",
    "    # The following code is to prepare for the SE calculation\n",
    "    # We need to use the correct first derivative for each observation, not the average derivative\n",
    "    Sigma_hat_variance_subsample = mse_first_derivative_per_obs(beta_subsample, y_subsample, x_subsample).T @\\\n",
    "                                    mse_first_derivative_per_obs(beta_subsample, y_subsample, x_subsample)/k_N\n",
    "\n",
    "    V_subsample = second_derivative_subsample\n",
    "\n",
    "\n",
    "    return beta_subsample, second_derivative_subsample, Sigma_hat_variance_subsample, V_subsample\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7772e7c-7091-484b-b51e-24c643df94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the least square approximation\n",
    "def LSA(beta,beta_subsample,second_derivative_subsample,lamda=0):\n",
    "    approx = 0\n",
    "    m_N = len(beta_subsample)\n",
    "    for i in range(0,m_N):\n",
    "        #iterate through m_N subsamples\n",
    "        approx += (beta-beta_subsample[i]).transpose()@second_derivative_subsample[i]@(beta-beta_subsample[i])\n",
    "    approx = approx/m_N\n",
    "    weights = np.mean(beta_subsample, axis=0)\n",
    "    #weights[:]*= weights\n",
    "    return approx + lamda*sum(abs(beta)/np.abs(weights)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6237e2d8-900e-4072-b5be-ce70c7552052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function collects the information from each subsample in a list\n",
    "def subbag(k_N, m_N, f, N, beta_true, e_noise = 1):\n",
    "    # First, generate a full sample\n",
    "    # Set true parameters\n",
    "    # beta_true = np.concatenate([\n",
    "    #     # setting boundaries to avoid true beta close to 0. The current set is 0.25\n",
    "    #         np.r_[np.linspace(-1, -0.25, 6), np.linspace(0.25, 1, 6)],\n",
    "    #         np.zeros(p - 12)])\n",
    "    p = beta_true.shape[0]\n",
    "    # # Create a csv file to store the simulated data\n",
    "    # with open(file_name, mode='w',newline='') as file:\n",
    "    #     f_writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    #     header = [\"y\"] + [f\"x{i}\" for i in range(1, p + 1)]\n",
    "    #     f_writer.writerow(header)\n",
    "        \n",
    "    # generate the p by p Toeplitz covariance matrix\n",
    "    rho = 0.5\n",
    "    idx = np.arange(p)\n",
    "    Sigma = rho ** np.abs(idx[:, None] - idx[None, :])\n",
    "    \n",
    "    # Generate correlated covariates\n",
    "    x = np.random.multivariate_normal(\n",
    "    mean=np.zeros(p),\n",
    "    cov=Sigma,\n",
    "    size=N\n",
    "    )   # shape: (N, p)\n",
    "\n",
    "    # Generate all noise at once\n",
    "    eps = np.random.normal(\n",
    "    loc=0,\n",
    "    scale=e_noise**0.5, # Note its SD not variance\n",
    "    size=N\n",
    "    )   # shape: (N,)\n",
    "\n",
    "    y = x @ beta_true + eps\n",
    "\n",
    "    simu_data =  np.hstack((y[:, None], x))\n",
    "    # Create lists to collect the information\n",
    "    beta_subsample_list = []\n",
    "    second_derivative_subsample_list =[]\n",
    "    Sigma_hat_variance_subsample_list = []\n",
    "    V_subsample_list = []\n",
    "    \n",
    "    for i in range(0,m_N):\n",
    "        subsample = simu_data[random.sample(range(1,len(simu_data)), k = k_N)]\n",
    "        # the result from the above function\n",
    "        #result = subsample_estimate(subsample = subsample, f = f, beta_true = beta_true)\n",
    "        y_subsample = subsample[:,0]\n",
    "        x_subsample = subsample[:,1:]\n",
    "        \n",
    "        # Obtain subsample estimates\n",
    "        beta_subsample = np.linalg.inv(x_subsample.T@x_subsample)@x_subsample.T@y_subsample\n",
    "        #minimize(f, beta_true, method = 'BFGS',    \n",
    "                                #args = (y_subsample,x_subsample)).x\n",
    "            # (x.tx)^(-1)xy\n",
    "        # Obtain capital Sigma for linear regression\n",
    "        second_derivative_subsample = mse_second_derivative(beta_subsample, y_subsample, x_subsample)\n",
    "    \n",
    "        # The following code is to prepare for the SE calculation\n",
    "        # We need to use the correct first derivative for each observation, not the average derivative\n",
    "        Sigma_hat_variance_subsample = mse_first_derivative_per_obs(beta_subsample, y_subsample, x_subsample).T @\\\n",
    "                                        mse_first_derivative_per_obs(beta_subsample, y_subsample, x_subsample)/k_N \n",
    "        \n",
    "        beta_subsample_list += [beta_subsample]\n",
    "        second_derivative_subsample_list += [second_derivative_subsample]\n",
    "        Sigma_hat_variance_subsample_list += [Sigma_hat_variance_subsample]\n",
    "        V_subsample_list += [second_derivative_subsample]\n",
    "    return beta_subsample_list, second_derivative_subsample_list, Sigma_hat_variance_subsample_list, V_subsample_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc71503-586f-4de2-a4e7-79d76263fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that selects the best lambda\n",
    "def SBIC(k_N, m_N, result, initial_value, lamda_constant = 1, interval = 0.0000001, scale = True):\n",
    "    BIC_min = float('inf')\n",
    "    for log_scale in range(0, int(-np.log10(interval))):\n",
    "        lamda = lamda_constant * 10 ** (-log_scale)\n",
    "        alpha = (k_N * m_N)/N\n",
    "        estimate = minimize(LSA, initial_value, method = 'Powell', args = (result[0], result[1], lamda)).x\n",
    "        df = sum(abs(estimate) > 10e-16)\n",
    "        if scale == True:\n",
    "            BIC = k_N * LSA(estimate, result[0], result[1], lamda = lamda) + df * np.log(N)\n",
    "        if scale == False:\n",
    "            BIC = LSA(estimate, result[0], result[1],lamda = lamda) + df * np.log(N)\n",
    "        if BIC < BIC_min:\n",
    "            BIC_min = BIC\n",
    "            lamda_min = lamda\n",
    "            estimate_optimal = estimate\n",
    "    return BIC_min, lamda_min, estimate_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f409d5e-4a39-46b1-a5b4-dd4112188a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Linear Regression\n",
    "\n",
    "# N = 500000\n",
    "# beta = np.array([3,1.5,0,0,2,0,0,0])\n",
    "# with open('sim linear data_N=500000_2.csv', mode='w',newline='') as file:\n",
    "#     f_writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#     f_writer.writerow([\"y\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\"])\n",
    "#     for n in range(0,N):\n",
    "#         x = np.random.normal(0,1,8)\n",
    "#         y = x@beta + np.random.normal(0,1,1)\n",
    "#         f_writer.writerow(y.tolist()+x.tolist())\n",
    "# file.close()\n",
    "# simu_data=np.genfromtxt('sim linear data_N=500000_2.csv', delimiter=',')\n",
    "# y=simu_data[1:,0]\n",
    "# x=simu_data[1:,1:]\n",
    "# beta_OLS=minimize(mse, beta, method=\"Powell\",args=(y,x)).x\n",
    "# beta_adaptive=minimize(adaptive, np.array([3,1.5,0,0,2,0,0,0]), method=\"Powell\",args=(mse,y,x,beta_OLS,0.01)).x\n",
    "# beta_adaptive\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18544c4-5faf-48ed-88a4-8a6468a3e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_saver(k_N, m_N, N, e_noise = 1, p = 200): # e_noise is the variance of error term in linear regression\n",
    "    alpha=(k_N * m_N)/N\n",
    "    beta_true = np.concatenate([\n",
    "            np.r_[np.linspace(-1, -0.5, 6), np.linspace(0.5, 1, 6)],\n",
    "            np.zeros(p - 12)])\n",
    "    # # Covariance matrix\n",
    "    # rho = 0.5\n",
    "    # idx = np.arange(p)\n",
    "    # Sigma = rho ** np.abs(idx[:, None] - idx[None, :])\n",
    "    # SNR = beta_true.T@Sigma@beta_true/e_noise\n",
    "    # prepare writing for subsample results\n",
    "    # If the summary file does not exist, create a new one\n",
    "    file_name = '../result/N=' + str(N) + '_k_N='+str(k_N)+'_'+'m_N='+str(m_N)+'_'+'p='+str(p)+'_e_noise=' +\\\n",
    "                str(e_noise)+            '_.csv'\n",
    "    if (not (os.path.exists(file_name))):\n",
    "        with open(file_name, mode='w',newline='') as f:\n",
    "            f_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            header = [\"index\"]\n",
    "            \n",
    "            # lasso betas\n",
    "            #header += [f\"lasso beta_{i}\" for i in range(1, p + 1)]\n",
    "            \n",
    "            # lasso true betas\n",
    "            header += [f\"lasso_true beta_{i}\" for i in range(1, p + 1)]\n",
    "            \n",
    "            # subsample SEs (only beta indices you want)\n",
    "            subsample_betas = list(range(1,13))\n",
    "            #for k in range(1, 6):   # SE1 ... SE5\n",
    "            header += [f\"subsample SE beta_{i}\" for i in subsample_betas]\n",
    "            \n",
    "            # CIs\n",
    "            #for k in range(1, 6):   # CI1 ... CI5\n",
    "            header += [f\"CI beta_{i}\" for i in subsample_betas]\n",
    "            \n",
    "            # scalars\n",
    "            header += [\n",
    "                #\"BIC_min\", \"lamda_min\",\n",
    "                \"BIC_min_true\", \"lamda_min_true\",\n",
    "                \"time1\", \"time2\",\n",
    "                \"memory\"\n",
    "            ]\n",
    "            f_writer.writerow(header)\n",
    "            \n",
    "    # Simulation start writing into the corresponding files            \n",
    "    with open(file_name, mode = 'a',newline = '') as f:\n",
    "        \n",
    "        f_writer = csv.writer(f, delimiter = ',', quotechar = '\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        for i in range(0,10):\n",
    "            \n",
    "            random.seed(rep_ind_current+i)\n",
    "            np.random.seed(rep_ind_current+i)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            # obtain the collection from subbag files\n",
    "            result = subbag(#'sim data_N=' + str(N) + '_' + str(rep_ind_current+i) + '_p=' + str(p) + '.csv',\n",
    "                            k_N, m_N, mse, N, beta_true, e_noise)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Simple average of subbagging estimates\n",
    "            estimate = np.mean(result[0], axis = 0)\n",
    "\n",
    "            # start_time1 = time.time()\n",
    "            # # LSA minmizer; we set lambda small to avoid potential bias for now\n",
    "            # # The optimizer method Powerll can give exactly value of 0 when intial value is true beta\n",
    "            # # For time comparsion to the tuning parameter of lambda\n",
    "            # estimate_lasso = minimize(LSA, beta_true, method='Powell',args=(result[0],result[1],0.00001)).x\n",
    "            # end_time1 = time.time()\n",
    "\n",
    "            # Lasso uses subbagging average as initial value\n",
    "            start_time2 = time.time()\n",
    "            lasso_result = SBIC(k_N, m_N, result, initial_value = beta_true)\n",
    "            estimate_lasso_true = lasso_result[2]\n",
    "            BIC_min_true = lasso_result[0]\n",
    "            lamda_min_true = lasso_result[1]\n",
    "            end_time2 = time.time()\n",
    "\n",
    "            # Sandwich matrix SE calculation\n",
    "            SE1_subsample = np.sqrt(((1 + 1/alpha)/N * np.linalg.inv(np.mean(result[3], axis = 0)[:12, :12]).T @ \\\n",
    "                        np.mean(result[2], axis = 0)[:12, :12] @\\\n",
    "                        np.linalg.inv(np.mean(result[3], axis = 0)[:12, :12]))[np.arange(12), np.arange(12)])\n",
    "\n",
    "            \n",
    "            # Coverage of confidence interval based on the SE\n",
    "            CI1_subsample = (estimate_lasso_true[:12] + norm.ppf(0.975) * SE1_subsample > beta_true[0:12]) * (estimate_lasso_true[:12] - norm.ppf(0.975) * SE1_subsample < beta_true[0:12])\n",
    "            \n",
    "            f_writer.writerow(([rep_ind_current+i]) + \n",
    "                              estimate_lasso_true.tolist() +\n",
    "                              SE1_subsample.tolist() +\n",
    "                              CI1_subsample.astype(int).tolist() +\n",
    "                              ([BIC_min_true]) +\n",
    "                              ([lamda_min_true]) +\n",
    "                              #[end_time1 - start_time1 + end_time - start_time] +\n",
    "                              [end_time2 - start_time2 + end_time - start_time] \n",
    "                             )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9965f705-eab1-4592-ac31-3c8cb1131577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.9071875"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Signal-noise-ratio\n",
    "p=12\n",
    "beta_true = np.concatenate([\n",
    "        np.r_[np.linspace(-1, -0.5, 6), np.linspace(0.5, 1, 6)],\n",
    "        np.zeros(p - 12)])\n",
    "rho = 0.5\n",
    "idx = np.arange(p)\n",
    "Sigma = rho ** np.abs(idx[:, None] - idx[None, :])\n",
    "beta_true.T@Sigma@beta_true\n",
    "\n",
    "# For ratio = 2\n",
    "beta_true.T@Sigma@beta_true/2\n",
    "\n",
    "# For ratio = 0.5\n",
    "beta_true.T@Sigma@beta_true/0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbde4d1-d6df-400c-be62-a0ba89b36eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Linear regression\n",
    "# # For test only\n",
    "N = 50000\n",
    "alpha = 1\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, e_noise = 1, p =15)\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, e_noise = 1, p =15)\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, e_noise = 1, p =15)\n",
    "\n",
    "N = 500000\n",
    "alpha = 0.5\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/2, p = 30)\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/0.5, p = 30)\n",
    "\n",
    "sim_saver(k_N=int(N**(1/3+1/2)),m_N=(int(alpha * N/(N**(1/3+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/2, p = 30)\n",
    "sim_saver(k_N=int(N**(1/3+1/2)),m_N=(int(alpha * N/(N**(1/3+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/0.5, p = 30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "alpha = 1\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/2, p = 30)\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/0.5, p = 30)\n",
    "\n",
    "sim_saver(k_N=int(N**(1/3+1/2)),m_N=(int(alpha * N/(N**(1/3+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/2, p = 30)\n",
    "sim_saver(k_N=int(N**(1/3+1/2)),m_N=(int(alpha * N/(N**(1/3+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/0.5, p = 30)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b976d-5928-4018-8b7c-8c9cacfbd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000000\n",
    "alpha = 0.5\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/2, p = 30)\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/0.5, p = 30)\n",
    "\n",
    "sim_saver(k_N=int(N**(1/3+1/2)),m_N=(int(alpha * N/(N**(1/3+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/2, p = 30)\n",
    "sim_saver(k_N=int(N**(1/3+1/2)),m_N=(int(alpha * N/(N**(1/3+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/0.5, p = 30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "alpha = 1\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/2, p = 30)\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/0.5, p = 30)\n",
    "\n",
    "sim_saver(k_N=int(N**(1/3+1/2)),m_N=(int(alpha * N/(N**(1/3+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/2, p = 30)\n",
    "sim_saver(k_N=int(N**(1/3+1/2)),m_N=(int(alpha * N/(N**(1/3+1/2)))+1), N = N, e_noise = beta_true.T@Sigma@beta_true/0.5, p = 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f793d00-f857-4672-b694-cdbe99059a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ef95b-5840-4211-9ffa-32a5b532d3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
