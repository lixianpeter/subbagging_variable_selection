{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06dc05d4-2cf9-4a72-a1fd-5b92ff400edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#rep_ind_current = 1\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# The following are the commonly used packages\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "import random \n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f27a4a1-344a-4542-bf70-13051d4ccebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some loss functions\n",
    "\n",
    "def adaptive(beta,f,y,x,beta_0,lamda):\n",
    "    return f(beta,y,x)+lamda*sum(abs(beta)/abs(beta_0))\n",
    "\n",
    "# def var_of_mse_grad_linear(x, y, beta):\n",
    "#     n, p =x.shape\n",
    "#     r = y - x @ beta                         # residuals, (n,)\n",
    "#     g_i = -2 * x * r[:, None]/len(y)              # (n,p), each row is g_i(beta)\n",
    "\n",
    "#     gbar = g_i.mean(axis=0)                  # (p,)\n",
    "#     centered = g_i - gbar[None, :]           # (n,p)\n",
    "\n",
    "#     cov_gi = centered.T @ centered / (n - 1) # (p,p) sample covariance of g_i\n",
    "#     return cov_gi\n",
    "\n",
    "def first_derivative(f,beta,*args):\n",
    "    grad=np.zeros(len(beta))\n",
    "    i=0\n",
    "    for coef in beta:\n",
    "        h=np.zeros(len(beta))\n",
    "        h[i]=1e-4\n",
    "        grad[i]=(f(beta+h,*args)-f(beta,*args))/1e-4\n",
    "        i+=1\n",
    "    return grad\n",
    "    \n",
    "def second_derivative(f,beta,*args):\n",
    "    hess=[]\n",
    "    for i in range(0,len(beta)):\n",
    "        h=np.zeros(len(beta))\n",
    "        h[i]=1e-4\n",
    "        hess+=[((first_derivative(f,beta+h,*args)-first_derivative(f,beta,*args))/1e-4)]\n",
    "    return np.stack(hess)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e0f1c2-cd0d-41ae-b385-2afcf1841ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For linear regression\n",
    "# def mse(beta,y,x):\n",
    "#     return sum((y-x@beta)**2)/len(y)\n",
    "    \n",
    "# def mse_first_derivative(beta,y,x):\n",
    "#     return -2*(x.T@(y-x@beta))/len(y)\n",
    "\n",
    "# def mse_first_derivative_per_obs(beta, y, x):\n",
    "#     \"\"\"\n",
    "#     Returns per-observation first derivatives g_i(beta)\n",
    "#     Shape: (m, p)\n",
    "#     \"\"\"\n",
    "#     r = y - x @ beta                    # (m,)\n",
    "#     return -2 * x * r[:, None]          # (m, p)\n",
    "\n",
    "# def mse_second_derivative(beta,y,x):\n",
    "#     return 2*x.T@x/len(y)\n",
    "\n",
    "# For logistic regression\n",
    "def logistic(z):\n",
    "    return np.exp(z) / (1 + np.exp(z))\n",
    "    \n",
    "def logistic_likelihood(beta,y,x):\n",
    "    X=x\n",
    "    Y=y\n",
    "    p=logistic(X@beta)\n",
    "    return -sum((Y*np.log(p)+(1-Y)*np.log(1-p)))/len(y)\n",
    "\n",
    "def logistic_first_derivative(beta,y,x):\n",
    "    Y=y\n",
    "    X=x\n",
    "    p=logistic(X@beta)\n",
    "    return -(Y@X-p@X)/len(y)\n",
    "\n",
    "def logistic_first_derivative_per_obs(beta, y, x):\n",
    "    X = x\n",
    "    Y = y\n",
    "    p = logistic(X @ beta)          \n",
    "    return X * (p - Y)[:, None]    \n",
    "\n",
    "def logistic_second_derivative(beta,y,x):\n",
    "    Y=y\n",
    "    X=x\n",
    "    p=logistic(X@beta)\n",
    "    return X.transpose()*(p*(1-p))@X/len(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b2adb6a-3e71-420d-bda8-9461db4b41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function is for a subsample's estimation\n",
    "\n",
    "\n",
    "# def subsample_estimate(subsample, f, beta_true = None): #  f is the loss; p is the dimension\n",
    "\n",
    "#     # extract one subsample\n",
    "#     #simu_data = pd.read_csv(file_name, header = 0).to_numpy()\n",
    "#     y_subsample = subsample[:,0]\n",
    "#     x_subsample = subsample[:,1:]\n",
    "#     p = x_subsample.shape[1]\n",
    "#     if (beta_true.all() == None):\n",
    "#         beta_true = np.zeros(p)\n",
    "#     k_N = len(y_subsample)\n",
    "\n",
    "\n",
    "#     clf = LogisticRegression(\n",
    "#                             penalty=\"none\",\n",
    "#                             solver=\"lbfgs\",\n",
    "#                             fit_intercept=False,\n",
    "#                             max_iter=5000,\n",
    "#                             tol=1e-10\n",
    "#                             )\n",
    "#     clf.fit(x_subsample, y_subsample)\n",
    "#     # obtain subsample estimates\n",
    "#     beta_subsample = clf.coef_.ravel()\n",
    "\n",
    "#     # obtain captial Sigma for logistic regession\n",
    "#     second_derivative_subsample = logistic_second_derivative(beta_subsample, y_subsample, x_subsample)\n",
    "\n",
    "#     # We need to use the correct first derivative for each observation, not the average derivative\n",
    "#     Sigma_hat_variance_subsample = logistic_first_derivative_per_obs(beta_subsample, y_subsample, x_subsample).T @\\\n",
    "#                             logistic_first_derivative_per_obs(beta_subsample, y_subsample, x_subsample)/k_N\n",
    "    \n",
    "#     V_subsample = second_derivative_subsample\n",
    "#     #V_true = logistic_second_derivative(beta_true, y_subsample, x_subsample)\n",
    "    \n",
    "    \n",
    "\n",
    "#     return beta_subsample, second_derivative_subsample, Sigma_hat_variance_subsample, V_subsample\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7772e7c-7091-484b-b51e-24c643df94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the least square approximation\n",
    "def LSA(beta,beta_subsample,second_derivative_subsample,lamda=0):\n",
    "    approx=0\n",
    "    m_N=len(beta_subsample)\n",
    "    for i in range(0,m_N):\n",
    "        #iterate through m_N subsamples\n",
    "        approx += (beta-beta_subsample[i]).transpose()@second_derivative_subsample[i]@(beta-beta_subsample[i])\n",
    "    approx = approx/m_N\n",
    "    weights = np.mean(beta_subsample, axis=0)\n",
    "    #weights[:]*= weights\n",
    "    return approx + lamda*sum(abs(beta)/np.abs(weights)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6237e2d8-900e-4072-b5be-ce70c7552052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function collect the information from each subsample in a list\n",
    "\n",
    "def subbag(k_N, m_N, f, N, beta_true):\n",
    "    p = beta_true.shape[0]\n",
    "    # generate the Toeplitz covariance matrix\n",
    "    rho = 0.5\n",
    "    idx = np.arange(p)\n",
    "    Sigma = rho ** np.abs(idx[:, None] - idx[None, :])\n",
    "\n",
    "    # Generate correlated covariates\n",
    "    x = np.random.multivariate_normal(\n",
    "    mean=np.zeros(p),\n",
    "    cov=Sigma,\n",
    "    size=N\n",
    "    )   # shape: (N, p)\n",
    "\n",
    "    # logistic model: P(y=1|x) = sigmoid(x @ beta)\n",
    "    z = x @ beta_true\n",
    "    prob = logistic(z)\n",
    "    y = np.random.binomial(1, prob)  # shape (1,)\n",
    "\n",
    "    simu_data =  np.hstack((y[:, None], x))\n",
    "    # create lists to collect the information\n",
    "    beta_subsample_list = []\n",
    "    second_derivative_subsample_list =[]\n",
    "    Sigma_hat_variance_subsample_list = []\n",
    "    #Sigma_hat_variance_true = []\n",
    "    V_subsample_list = []\n",
    "    #V_true = []\n",
    "    for i in range(0,m_N):\n",
    "        subsample = simu_data[random.sample(range(1,len(simu_data)), k = k_N)]\n",
    "        # the result from the above function\n",
    "        #result = subsample_estimate(subsample = subsample, f = f, beta_true = beta_true)\n",
    "        y_subsample = subsample[:,0]\n",
    "        x_subsample = subsample[:,1:]\n",
    "        clf = LogisticRegression(\n",
    "                                penalty=\"none\",\n",
    "                                solver=\"lbfgs\",\n",
    "                                fit_intercept=False,\n",
    "                                max_iter=5000,\n",
    "                                tol=1e-10\n",
    "                                )\n",
    "        clf.fit(x_subsample, y_subsample)\n",
    "        # obtain subsample estimates\n",
    "        beta_subsample = clf.coef_.ravel()\n",
    "    \n",
    "        # obtain captial Sigma for logistic regession\n",
    "        second_derivative_subsample = logistic_second_derivative(beta_subsample, y_subsample, x_subsample)\n",
    "    \n",
    "        # We need to use the correct first derivative for each observation, not the average derivative\n",
    "        Sigma_hat_variance_subsample = logistic_first_derivative_per_obs(beta_subsample, y_subsample, x_subsample).T @\\\n",
    "                                logistic_first_derivative_per_obs(beta_subsample, y_subsample, x_subsample)/k_N\n",
    "\n",
    "        \n",
    "        beta_subsample_list += [beta_subsample]\n",
    "        second_derivative_subsample_list += [second_derivative_subsample]\n",
    "        Sigma_hat_variance_subsample_list += [Sigma_hat_variance_subsample]\n",
    "        V_subsample_list += [second_derivative_subsample]\n",
    "        \n",
    "    return beta_subsample_list, second_derivative_subsample_list, Sigma_hat_variance_subsample_list, V_subsample_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc71503-586f-4de2-a4e7-79d76263fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function that selects the best lambda\n",
    "def SBIC(k_N, m_N, result, initial_value, lamda_constant = 1, interval = 0.00000001, scale = True):\n",
    "    BIC_min = float('inf')\n",
    "    # beta_true = np.array([3,1.5,0,0,2,0,0,0])\n",
    "    # beta_subbagging_average = np.mean(result[0], axis = 0)\n",
    "    for log_scale in range(0, int(-np.log10(interval))):\n",
    "        lamda = lamda_constant * 10 ** (-log_scale)\n",
    "        alpha = (k_N * m_N)/N\n",
    "        estimate = minimize(LSA, initial_value, method = \"Powell\", args = (result[0], result[1], lamda)).x\n",
    "        df = sum(abs(estimate) > 10e-16)\n",
    "        if scale == True:\n",
    "            BIC = k_N * LSA(estimate, result[0], result[1], lamda = lamda) + df * np.log(N)\n",
    "        if scale == False:\n",
    "            BIC = LSA(estimate, result[0], result[1],lamda = lamda) + df * np.log(N)\n",
    "        if BIC < BIC_min:\n",
    "            BIC_min = BIC\n",
    "            lamda_min = lamda\n",
    "            estimate_optimal = estimate\n",
    "    return BIC_min, lamda_min, estimate_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f409d5e-4a39-46b1-a5b4-dd4112188a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Linear Regression\n",
    "\n",
    "# N = 500000\n",
    "# beta = np.array([3,1.5,0,0,2,0,0,0])\n",
    "# with open('sim linear data_N=500000_2.csv', mode='w',newline='') as file:\n",
    "#     f_writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#     f_writer.writerow([\"y\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\"])\n",
    "#     for n in range(0,N):\n",
    "#         x = np.random.normal(0,1,8)\n",
    "#         y = x@beta + np.random.normal(0,1,1)\n",
    "#         f_writer.writerow(y.tolist()+x.tolist())\n",
    "# file.close()\n",
    "# simu_data=np.genfromtxt('sim linear data_N=500000_2.csv', delimiter=',')\n",
    "# y=simu_data[1:,0]\n",
    "# x=simu_data[1:,1:]\n",
    "# beta_OLS=minimize(mse, beta, method=\"Powell\",args=(y,x)).x\n",
    "# beta_adaptive=minimize(adaptive, np.array([3,1.5,0,0,2,0,0,0]), method=\"Powell\",args=(mse,y,x,beta_OLS,0.01)).x\n",
    "# beta_adaptive\n",
    "\n",
    "\n",
    "\n",
    "# beta_true = np.concatenate([\n",
    "#             np.linspace(-1, 1, 12),\n",
    "#             np.zeros(p - 12)])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18544c4-5faf-48ed-88a4-8a6468a3e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_saver(k_N, m_N, N, p = 200):\n",
    "    alpha=(k_N * m_N)/N\n",
    "    beta_true = np.concatenate([\n",
    "            np.r_[np.linspace(-1, -0.5, 6), np.linspace(0.5, 1, 6)],\n",
    "            np.zeros(p - 12)])\n",
    "    # prepare writing for subsample results\n",
    "    # SE_fullsample=np.sqrt((1+1/alpha)*np.linalg.inv(second_derivative(mse,beta,y,x)[[0,1,4],:][:,[0,1,4]])[[0,1,2],[0,1,2]])\n",
    "    # If summary file not exist, create a new one\n",
    "    file_name = '../result/N=' + str(N) + '_k_N='+str(k_N)+'_'+'m_N='+str(m_N)+'_'+'p='+str(p)+ '_.csv'\n",
    "    if (not (os.path.exists(file_name))):\n",
    "        with open(file_name, mode='w',newline='') as f:\n",
    "            f_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            header = [\"index\"]\n",
    "            \n",
    "            # lasso betas\n",
    "            #header += [f\"lasso beta_{i}\" for i in range(1, p + 1)]\n",
    "            \n",
    "            # lasso true betas\n",
    "            header += [f\"lasso_true beta_{i}\" for i in range(1, p + 1)]\n",
    "            \n",
    "            # subsample SEs (only beta indices you want)\n",
    "            subsample_betas = list(range(1,13))\n",
    "            #for k in range(1, 6):   # SE1 ... SE5\n",
    "            header += [f\"subsample SE beta_{i}\" for i in subsample_betas]\n",
    "            \n",
    "            # CIs\n",
    "            #for k in range(1, 6):   # CI1 ... CI5\n",
    "            header += [f\"CI beta_{i}\" for i in subsample_betas]\n",
    "            \n",
    "            # scalars\n",
    "            header += [\n",
    "                #\"BIC_min\", \"lamda_min\",\n",
    "                \"BIC_min_true\", \"lamda_min_true\",\n",
    "                \"time1\" #\"time2\",\n",
    "                #\"memory\"\n",
    "            ]\n",
    "            f_writer.writerow(header)\n",
    "            \n",
    "    # simulation start writing into the corresponding files            \n",
    "    with open(file_name, mode = 'a',newline = '') as f:\n",
    "        \n",
    "        f_writer = csv.writer(f, delimiter = ',', quotechar = '\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        for i in range(0,10):\n",
    "            \n",
    "            random.seed(rep_ind_current+i)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            # obtain the collection from subbag files\n",
    "            result = subbag(k_N, m_N, logistic_likelihood, N, beta_true)\n",
    "            \n",
    "            \n",
    "            # Simple average of subbagging estimates\n",
    "            #estimate = np.mean(result[0], axis = 0)\n",
    "\n",
    "            #start_time1 = time.time()\n",
    "            # LSA minmizer; we set lambda small to avoid potential bias for now\n",
    "            # The optimizer method Powerll can give exactly value of 0 when intial value is true beta\n",
    "            # For time comparsion to the tuning parameter of lambda\n",
    "            #estimate_lasso = minimize(LSA, beta_true, method='Powell',args=(result[0],result[1],0.00001)).x\n",
    "            #end_time1 = time.time()\n",
    "\n",
    "            # Lasso uses subbagging average as initial value\n",
    "            #start_time2 = time.time()\n",
    "            lasso_result = SBIC(k_N, m_N, result, initial_value = beta_true)\n",
    "            estimate_lasso_true = lasso_result[2]\n",
    "            BIC_min_true = lasso_result[0]\n",
    "            lamda_min_true = lasso_result[1]\n",
    "            end_time = time.time()\n",
    "\n",
    "            # First kind of SE calculation; i.e., based on the subbagging\n",
    "            #SE1_subsample = np.sqrt(k_N * (1 + 1/alpha) * ((np.array(result[0]) - estimate).T@(np.array(result[0]) - estimate))[[0,1,4],[0,1,4]]/m_N/N)\n",
    "\n",
    "            # Other sandwitch matrix SE calculation\n",
    "            # SE1_subsample = np.sqrt((1 + 1/alpha)/N * np.linalg.inv(np.mean(result[2], axis = 0)).T @ \\\n",
    "            #                         np.mean(result[4], axis = 0) @\\\n",
    "            #                         np.linalg.inv(np.mean(result[4], axis = 0)))[[0,1,4],[0,1,4]]\n",
    "            SE1_subsample = np.sqrt(((1 + 1/alpha)/N * np.linalg.inv(np.mean(result[3], axis = 0)[:12, :12]).T @ \\\n",
    "                        np.mean(result[2], axis = 0)[:12, :12] @\\\n",
    "                        np.linalg.inv(np.mean(result[3], axis = 0)[:12, :12]))[np.arange(12), np.arange(12)])\n",
    "            # SE4_subsample = np.sqrt((1 + 1/alpha)/N * np.linalg.inv(np.mean(result[3], axis = 0)).T @ \\\n",
    "            #                         np.mean(result[5], axis = 0) @\\\n",
    "            #                         np.linalg.inv(np.mean(result[3], axis = 0)))[[0,1,4],[0,1,4]]\n",
    "            # SE5_subsample = np.sqrt((1 + 1/alpha)/N * np.linalg.inv(np.mean(result[3], axis = 0)[np.ix_([0,1,4],[0,1,4])]).T @ \\\n",
    "            #                         np.mean(result[5], axis = 0)[np.ix_([0,1,4],[0,1,4])] @\\\n",
    "            #                         np.linalg.inv(np.mean(result[3], axis = 0))[np.ix_([0,1,4],[0,1,4])])[[0,1,2],[0,1,2]]\n",
    "            # # New SE calculation\n",
    "            #SE6_subsample = np.sqrt((1 + 1/alpha)/N * np.mean(result[4], axis=0)[[0,1,4],[0,1,4]])\n",
    "\n",
    "            \n",
    "            # Coverage of confidence interval based on the SE\n",
    "            CI1_subsample = (estimate_lasso_true[:12] + norm.ppf(0.975) * SE1_subsample > beta_true[0:12]) * (estimate_lasso_true[:12] - norm.ppf(0.975) * SE1_subsample < beta_true[0:12])\n",
    "            # CI2_subsample = (estimate_lasso_true[[0,1,4]] + norm.ppf(0.975) * SE2_subsample > [3, 1.5, 2]) * (estimate_lasso_true[[0,1,4]] - norm.ppf(0.975) * SE2_subsample < [3, 1.5, 2])\n",
    "            # CI3_subsample = (estimate_lasso_true[[0,1,4]] + norm.ppf(0.975) * SE3_subsample > [3, 1.5, 2]) * (estimate_lasso_true[[0,1,4]] - norm.ppf(0.975) * SE3_subsample < [3, 1.5, 2])            \n",
    "            # CI4_subsample = (estimate_lasso_true[[0,1,4]] + norm.ppf(0.975) * SE4_subsample > [3, 1.5, 2]) * (estimate_lasso_true[[0,1,4]] - norm.ppf(0.975) * SE4_subsample < [3, 1.5, 2])            \n",
    "            # CI5_subsample = (estimate_lasso_true[[0,1,4]] + norm.ppf(0.975) * SE5_subsample > [3, 1.5, 2]) * (estimate_lasso_true[[0,1,4]] - norm.ppf(0.975) * SE5_subsample < [3, 1.5, 2])            \n",
    "            # CI6_subsample = (estimate_lasso[[0,1,4]] + norm.ppf(0.975) * SE6_subsample > [3, 1.5, 2]) * (estimate[[0,1,4]] - norm.ppf(0.975) * SE6_subsample < [3, 1.5, 2])            \n",
    "            # CI7_subsample = (estimate[[0,1,4]] + 1.96 * SE2_subsample > [3, 1.5, 2]) * (estimate[[0,1,4]] - 1.96 * SE2_subsample < [3, 1.5, 2])            \n",
    "\n",
    "            \n",
    "            f_writer.writerow(([rep_ind_current+i]) + \n",
    "                              #estimate.tolist() + \n",
    "                              #estimate_lasso.tolist() +\n",
    "                              estimate_lasso_true.tolist() +\n",
    "                              SE1_subsample.tolist() +\n",
    "                              # SE2_subsample.tolist() + \n",
    "                              # SE3_subsample.tolist() +\n",
    "                              # SE4_subsample.tolist() +\n",
    "                              # SE5_subsample.tolist() +\n",
    "                              #SE6_subsample.tolist() +\n",
    "                              CI1_subsample.astype(int).tolist() +\n",
    "                              # CI2_subsample.astype(int).tolist() +\n",
    "                              # CI3_subsample.astype(int).tolist() +\n",
    "                              # CI4_subsample.astype(int).tolist() +\n",
    "                              # CI5_subsample.astype(int).tolist() +\n",
    "                              #CI6_subsample.astype(int).tolist() +\n",
    "                              #CI7_subsample.astype(int).tolist() +\n",
    "                              # ([BIC_min]) +\n",
    "                              # ([lamda_min]) +\n",
    "                              ([BIC_min_true]) +\n",
    "                              ([lamda_min_true]) +\n",
    "                              #[end_time1 - start_time1 + end_time - start_time] +\n",
    "                              [ end_time - start_time] \n",
    "                             )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bbde4d1-d6df-400c-be62-a0ba89b36eae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rep_ind_current' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-880627faff41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msim_saver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_N\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm_N\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-c49084843427>\u001b[0m in \u001b[0;36msim_saver\u001b[0;34m(k_N, m_N, N, p)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep_ind_current\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rep_ind_current' is not defined"
     ]
    }
   ],
   "source": [
    "# Logistics regression\n",
    "# For test\n",
    "N = 50000\n",
    "alpha = 1\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, p =15)\n",
    "\n",
    "\n",
    "\n",
    "N = 500000\n",
    "alpha = 0.5\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, p = 30)\n",
    "sim_saver(k_N=int(N**(1/3+1/2)),m_N=(int(alpha * N/(N**(1/3+1/2)))+1), N = N, p = 30)\n",
    "\n",
    "\n",
    "alpha = 1\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, p = 30)\n",
    "sim_saver(k_N=int(N**(1/3+1/2)),m_N=(int(alpha * N/(N**(1/3+1/2)))+1), N = N, p = 30)\n",
    "\n",
    "\n",
    "\n",
    "alpha = 2\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, p = 30)\n",
    "sim_saver(k_N=int(N**(1/3+1/2)),m_N=(int(alpha * N/(N**(1/3+1/2)))+1), N = N, p = 30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N = 1000000\n",
    "alpha = 0.5\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, p = 30)\n",
    "sim_saver(k_N=int(N**(1/3+1/2)),m_N=(int(alpha * N/(N**(1/3+1/2)))+1), N = N, p = 30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "alpha = 1\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, p = 30)\n",
    "sim_saver(k_N=int(N**(1/3+1/2)),m_N=(int(alpha * N/(N**(1/3+1/2)))+1), N = N, p = 30)\n",
    "\n",
    "\n",
    "alpha = 2\n",
    "sim_saver(k_N=int(N**(1/4+1/2)),m_N=(int(alpha * N/(N**(1/4+1/2)))+1), N = N, p = 30)\n",
    "sim_saver(k_N=int(N**(1/3+1/2)),m_N=(int(alpha * N/(N**(1/3+1/2)))+1), N = N, p = 30)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b976d-5928-4018-8b7c-8c9cacfbd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bf9eb8-8f17-4661-a0dd-11f88d6a7057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d789c-a3fa-4698-a957-8abf24e85eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98507a68-11bc-42f1-8fe8-67822d0ddd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3712a4-b68e-4f7b-8c33-2ec64e4e8df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c4480-9ec9-4de7-9a15-84727f74035f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9e8a41f6-ce6e-4147-afe8-9cf675d19340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459ab96c-0a42-4b57-88c6-8d021b397716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b913c21-381e-4138-aa4f-2410bd613fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89cd133-8e4f-45ed-9b23-f07f0f2353e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac57f462-8f25-4f90-ba9b-e07ce30a638c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
